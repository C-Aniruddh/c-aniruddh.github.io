---
layout: post
title:  "NOVA Home Automation (2016) - My first attempt at integrating Software and Hardware"
author: aniruddh
categories: [ Apps, Machine Learning, NLP, Robotics ]
image: https://aniruddh-chandratre-e877.netlify.app/static/ead35e6dad46aca7206b9d73d3b9fedd/f566d/18582289_1879632648729328_1803737816645017494_n-1.jpg
toc: true
---


## Motivation
In December 2016, Mark Zukerberg wrote a comprehensive note on how he built a personal home automation system throughout the year. After a careful analysis of the system, I felt an urge to try something similar, and so I got to work!

## Day 1
At this point, I only have an Arduino UNO along with a 16*2 LCD Display. With limited components, creating a full fledged system is going to be extremely difficult.  So I start off by building  a simple serial communication sketch in the Arduino IDE. At this point, all it can do is put a HIGH/ LOW signal to any pin as per the serial input. Now that this is ready, it's time to work on how exactly the Arduino will get the serial input.

I start off by building a simple Flask Server with a basic html page with a basic text field and a radio button. At this moment, the values from the html form are sent to the flask server and then the server uses pyserial to communicate with the Arduino Uno. However, this doesn't give the automation feel it is supposed to.

So I modified the flask endpoint to receive a natural sentence (such as "Turn on light in kitchen", "Turn off light in bedroom", etc) and process that. However, my NLP skills are not top-notch at this point to implement a full fledged entitiy extraction module which can understand the actions and the entities in a sentence.

Lucky for me, I have DialogFlow (Api.ai back then) to my rescue. I can simply train a simple NLP model for extracting action and entity from any sentence. Now that this is linked, I have to make sure my system understands that kitchen refers to PIN 10 and bedroom refers to PIN 12. For this I build a simple collection in a MongoDB which stores the PIN number for each room.


So right now, the system works like this :

1. I send a GET request at `/query` endpoint.
2. After parsing the sentence from the endpoint, I make a request to DialogFlow Servers with the sentence.
3. DialogFlow returns the action and the entity.
4. I query my database with the entity to get the `pin_number`.
5. With the `pin_number`, I can construct a serial command such as `10-ON` or `12-OFF`.
6. On my arduino, this serial command is parsed such that we can separate the action ( `ON` / `OFF` ) and the pin_number and then perform the appropriate action.

The current flow is quite easy. Now I need to make a voice recognition wrapper around the same. What would be the most cost effective and fastest way to achieve this?

So, I started off by building a simple Android Application which makes use of the Google Speech Recognition API and performs speech recognition on the app itself. Now all I have to do is to send a GET request with the sentence to my server at `/query`. Once this is set, I can start controlling lights with my voice!

In about two hours after I started off, I had a pretty simple, yet functional system ready at my disposal. Have a look at what could be done within 2 hours!

<iframe width="560" height="315" src="https://www.youtube.com/embed/zg_vk9lh_k8" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>


## Day 2
I have a simple home automation system in place. At this point it cannot really control any real lights throughout the house, but the basic structure is ready. All I have to do to get it interface with the lights throughout the house is to create a vast network of relays which can be controlled with my Arduino. However, I cannot keep using my laptop as the main processing unit. So all the functions (hosting the server, communicating with arduino and dialogflow) from the laptop are transferred to a Raspberry Pi 3, which is connected to the LAN network of my house.

It's time to add more functionality to the assistant, after all, its not just meant for controlling lights, but also as an interactive assistant which can help you get through the day! So by interfacing it with more and more APIs and core services, I can get the bot to respond to several queries such as : "what time is it?", "weather in Mumbai".

However, a true home automation agent is incomplete without music! At this point I have an account for Spotify, but Spotify does not let me stream music over its API, and that is disheartening. So what can be done? At this point, I have an old music library! So I gather all this music and import them into a new collection. Now I can simply use dialogflow and setup more actions! Its day 2 of NOVA, and I can get quite a few basic tasks done with simple voice commands!

<iframe width="560" height="315" src="https://www.youtube.com/embed/V6ELmX2gsdg" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

## One month later
At this point the entire project has turned into a guinea pig for me to test new technologies! At this point, I am quite interested in Natural Language Processing and generation of text. I start off by building `seq2seq` models for contextual conversations. NOVA at this point has an updated app, a new way to learn the music I like (via feedback), and a lot of new features!

<iframe width="560" height="315" src="https://www.youtube.com/embed/-odHbghxD2s" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>


## What now?
The project is sadly abandoned as of November 2020. However, back when it was under active development, it was a great oppotunity to learn how software and hardware can interact with each other on a basic level.